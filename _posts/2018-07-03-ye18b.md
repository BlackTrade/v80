---
title: 'Variable Selection via Penalized Neural Network: a Drop-Out-One Loss Approach'
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/ye18b/ye18b.pdf
url: http://proceedings.mlr.press/v80/ye2018b.html
abstract: We propose a variable selection method for high dimensional regression models,
  which allows for complex, nonlinear, and high-order interactions among variables.
  The proposed method approximates this complex system using a penalized neural network
  and selects explanatory variables by measuring their utility in explaining the variance
  of the response variable. This measurement is based on a novel statistic called
  Drop-Out-One Loss. The proposed method also allows (overlapping) group variable
  selection. We prove that the proposed method can select relevant variables and exclude
  irrelevant variables with probability one as the sample size goes to infinity, which
  is referred to as the Oracle Property. Experimental results on simulated and real
  world datasets show the efficiency of our method in terms of variable selection
  and prediction accuracy.
layout: inproceedings
id: ye18b
tex_title: 'Variable Selection via Penalized Neural Network: a Drop-Out-One Loss Approach'
firstpage: 5616
lastpage: 5625
page: 5616-5625
order: 5616
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Ye, Mao and Sun, Yan
author:
- given: Mao
  family: Ye
- given: Yan
  family: Sun
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v80/ye18b/ye18b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
