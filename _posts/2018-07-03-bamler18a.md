---
title: Improving Optimization in Models With Continuous Symmetry Breaking
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/bamler18a/bamler18a.pdf
url: http://proceedings.mlr.press/v80/bamler2018a.html
abstract: Many loss functions in representation learning are invariant under a continuous
  symmetry transformation. For example, the loss function of word embeddings (Mikolov
  et al., 2013) remains unchanged if we simultaneously rotate all word and context
  embedding vectors. We show that representation learning models for time series possess
  an approximate continuous symmetry that leads to slow convergence of gradient descent.
  We propose a new optimization algorithm that speeds up convergence using ideas from
  gauge theory in physics. Our algorithm leads to orders of magnitude faster convergence
  and to more interpretable representations, as we show for dynamic extensions of
  matrix factorization and word embedding models. We further present an example application
  of our proposed algorithm that translates modern words into their historic equivalents.
layout: inproceedings
id: bamler18a
tex_title: Improving Optimization in Models With Continuous Symmetry Breaking
firstpage: 432
lastpage: 441
page: 432-441
order: 432
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Bamler, Robert and Mandt, Stephan
author:
- given: Robert
  family: Bamler
- given: Stephan
  family: Mandt
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
