---
title: 'The Power of Interpolation: Understanding the Effectiveness of SGD in Modern
  Over-parametrized Learning'
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/ma18a/ma18a.pdf
url: http://proceedings.mlr.press/v80/ma2018a.html
abstract: 'In this paper we aim to formally explain the phenomenon of fast convergence
  of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key
  observation is that most modern learning architectures are over-parametrized and
  are trained to interpolate the data by driving the empirical loss (classification
  and regression) close to zero. While it is still unclear why these interpolated
  solutions perform well on test data, we show that these regimes allow for fast convergence
  of SGD, comparable in number of iterations to full gradient descent. For convex
  loss functions we obtain an exponential convergence bound for <em>mini-batch</em>
  SGD parallel to that for full gradient descent. We show that there is a critical
  batch size $m^*$ such that: (a) SGD iteration with mini-batch size $m\leq m^*$ is
  nearly equivalent to $m$ iterations of mini-batch size $1$ (<em>linear scaling regime</em>).
  (b) SGD iteration with mini-batch $m> m^*$ is nearly equivalent to a full gradient
  descent iteration (<em>saturation regime</em>). Moreover, for the quadratic loss,
  we derive explicit expressions for the optimal mini-batch and step size and explicitly
  characterize the two regimes above. The critical mini-batch size can be viewed as
  the limit for effective mini-batch parallelization. It is also nearly independent
  of the data size, implying $O(n)$ acceleration over GD per unit of computation.
  We give experimental evidence on real data which closely follows our theoretical
  analyses. Finally, we show how our results fit in the recent developments in training
  deep neural networks and discuss connections to adaptive rates for SGD and variance
  reduction.'
layout: inproceedings
id: ma18a
tex_title: 'The Power of Interpolation: Understanding the Effectiveness of {SGD} in
  Modern Over-parametrized Learning'
firstpage: 3325
lastpage: 3334
page: 3325-3334
order: 3325
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Ma, Siyuan and Bassily, Raef and Belkin, Mikhail
author:
- given: Siyuan
  family: Ma
- given: Raef
  family: Bassily
- given: Mikhail
  family: Belkin
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
