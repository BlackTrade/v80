---
title: 'Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments
  for Compressing Deep Convolutions'
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/wu18h/wu18h.pdf
url: http://proceedings.mlr.press/v80/wu2018h.html
abstract: Many existing compression approaches have been focused and evaluated on
  convolutional neural networks (CNNs) where fully-connected layers contain the most
  parameters (e.g., LeNet and AlexNet). However, the current trend of pushing CNNs
  deeper with convolutions has created a pressing demand to achieve higher compression
  gains on CNNs where convolutions dominate the parameter amount (e.g., GoogLeNet,
  ResNet and Wide ResNet). Further, convolutional layers always account for most energy
  consumption in run time. To this end, this paper investigates the relatively less-explored
  direction of compressing convolutional layers in deep CNNs. We introduce a novel
  spectrally relaxed k -means regularization, that tends to approximately make hard
  assignments of convolutional layer weights to K learned cluster centers during re-training.
  Compression is then achieved through weight-sharing, by only recording K cluster
  centers and weight assignment indexes. Our proposed pipeline, termed Deep k -Means,
  has well-aligned goals between re-training and compression stages. We further propose
  an improved set of metrics to estimate energy consumption of CNN hardware implementations,
  whose estimation results are verified to be consistent with previously proposed
  energy estimation tool extrapolated from actual hardware measurements. We have evaluated
  Deep k -Means in compressing several CNN models in terms of both compression ratio
  and energy consumption reduction, observing promising results without incurring
  accuracy loss.
layout: inproceedings
id: wu18h
tex_title: 'Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments
  for Compressing Deep Convolutions'
firstpage: 5359
lastpage: 5368
page: 5359-5368
order: 5359
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Wu, Junru and Wang, Yue and Wu, Zhenyu and Wang, Zhangyang and Veeraraghavan,
  Ashok and Lin, Yingyan
author:
- given: Junru
  family: Wu
- given: Yue
  family: Wang
- given: Zhenyu
  family: Wu
- given: Zhangyang
  family: Wang
- given: Ashok
  family: Veeraraghavan
- given: Yingyan
  family: Lin
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
