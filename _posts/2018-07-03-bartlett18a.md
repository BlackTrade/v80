---
title: Gradient descent with identity initialization efficiently learns positive definite
  linear transformations
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/bartlett18a/bartlett18a.pdf
url: http://proceedings.mlr.press/v80/bartlett2018a.html
abstract: 'We analyze algorithms for approximatinga function$f(x) = Φx$mapping $\Re^d$
  to $\Re^d$ using deep linearneural networks, i.e. that learn a function $h$ parameterizedby
  matrices $\Theta_1,...,\Theta_L$ and defined by$h(x) = \Theta_L \Theta_{L-1} ...
  \Theta_1 x$. We focuson algorithms that learn through gradient descent on the populationquadratic
  loss in the case that the distribution over the inputs isisotropic. We provide polynomial
  bounds on the number ofiterations for gradient descent to approximate theleast squares
  matrix $Φ$, in the case wherethe initial hypothesis $\Theta_1 = ... = \Theta_L =
  I$ has excess loss bounded by a small enoughconstant. On the other hand,we show
  that gradient descent fails to converge for$Φ$ whose distance from the identityis
  a larger constant, and we show that some formsof regularization toward the identity
  in each layer donot help. If $Φ$ is symmetric positive definite,we show that an
  algorithm that initializes $\Theta_i = I$learns an $ε$-approximation of $f$ using
  a number of updates polynomial in $L$,the condition number of $Φ$, and $\log(d/ε)$.
  In contrast, we showthat if the least squares matrix $Φ$ is symmetric and has anegative
  eigenvalue, then all members of a class of algorithmsthat perform gradient descent
  with identity initialization,and optionally regularize toward the identity in each
  layer, fail toconverge. We analyze an algorithm for the case that $Φ$ satisfies
  $u^{⊤}Φu > 0$ for all $u$, but may not be symmetric. This algorithm usestwo regularizers:
  one that maintains the invariant $u^{⊤} \Theta_L\Theta_{L-1} ... \Theta_1 u > 0$
  for all $u$, and another that “balances”$\Theta_1, ..., \Theta_L$ so that they have
  the same singular values.'
layout: inproceedings
id: bartlett18a
tex_title: Gradient descent with identity initialization efficiently learns positive
  definite linear transformations
firstpage: 520
lastpage: 529
page: 520-529
order: 520
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Bartlett, Peter and Helmbold, Dave and Long, Phil
author:
- given: Peter
  family: Bartlett
- given: Dave
  family: Helmbold
- given: Phil
  family: Long
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
