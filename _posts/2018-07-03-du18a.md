---
title: On the Power of Over-parametrization in Neural Networks with Quadratic Activation
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/du18a/du18a.pdf
url: http://proceedings.mlr.press/v80/du2018a.html
abstract: We provide new theoretical insights on why over-parametrization is effective
  in learning neural networks. For a $k$ hidden node shallow network with quadratic
  activation and $n$ training data points, we show as long as $ k \ge \sqrt{2n}$,
  over-parametrization enables local search algorithms to find a <em>globally</em>
  optimal solution for general smooth and convex loss functions. Further, despite
  that the number of parameters may exceed the sample size, using theory of Rademacher
  complexity, we show with weight decay, the solution also generalizes well if the
  data is sampled from a regular distribution such as Gaussian. To prove when $k\ge
  \sqrt{2n}$, the loss function has benign landscape properties, we adopt an idea
  from smoothed analysis, which may have other applications in studying loss surfaces
  of neural networks.
layout: inproceedings
id: du18a
tex_title: On the Power of Over-parametrization in Neural Networks with Quadratic
  Activation
firstpage: 1329
lastpage: 1338
page: 1329-1338
order: 1329
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Du, Simon and Lee, Jason
author:
- given: Simon
  family: Du
- given: Jason
  family: Lee
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
