---
title: Learning Compact Neural Networks with Regularization
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/oymak18a/oymak18a.pdf
url: http://proceedings.mlr.press/v80/oymak2018a.html
abstract: Proper regularization is critical for speeding up training, improving generalization
  performance, and learning compact models that are cost efficient. We propose and
  analyze regularized gradient descent algorithms for learning shallow neural networks.
  Our framework is general and covers weight-sharing (convolutional networks), sparsity
  (network pruning), and low-rank constraints among others. We first introduce covering
  dimension to quantify the complexity of the constraint set and provide insights
  on the generalization properties. Then, we show that proposed algorithms become
  well-behaved and local linear convergence occurs once the amount of data exceeds
  the covering dimension. Overall, our results demonstrate that near-optimal sample
  complexity is sufficient for efficient learning and illustrate how regularization
  can be beneficial to learn over-parameterized networks.
layout: inproceedings
id: oymak18a
tex_title: Learning Compact Neural Networks with Regularization
firstpage: 3966
lastpage: 3975
page: 3966-3975
order: 3966
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Oymak, Samet
author:
- given: Samet
  family: Oymak
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
