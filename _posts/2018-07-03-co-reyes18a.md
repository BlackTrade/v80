---
title: 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning
  with Trajectory Embeddings'
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/co-reyes18a/co-reyes18a.pdf
url: http://proceedings.mlr.press/v80/co-reyes2018a.html
abstract: In this work, we take a representation learning perspective on hierarchical
  reinforcement learning, where the problem of learning lower layers in a hierarchy
  is transformed into the problem of learning trajectory-level generative models.
  We show that we can learn continuous latent representations of trajectories, which
  are effective in solving temporally extended and multi-stage problems. Our proposed
  model, SeCTAR, draws inspiration from variational autoencoders, and learns latent
  representations of trajectories. A key component of this method is to learn both
  a latent-conditioned policy and a latent-conditioned model which are consistent
  with each other. Given the same latent, the policy generates a trajectory which
  should match the trajectory predicted by the model. This model provides a built-in
  prediction mechanism, by predicting the outcome of closed loop policy behavior.
  We propose a novel algorithm for performing hierarchical RL with this model, combining
  model-based planning in the learned latent space with an unsupervised exploration
  objective. We show that our model is effective at reasoning over long horizons with
  sparse rewards for several simulated tasks, outperforming standard reinforcement
  learning methods and prior methods for hierarchical reasoning, model-based planning,
  and exploration. This model provides a built-in prediction mechanism, by predicting
  the outcome of closed loop policy behavior. We propose a novel algorithm for performing
  hierarchical RL with this model, combining model-based planning in the learned latent
  space with an unsupervised exploration objective. We show that our model is effective
  at reasoning over long horizons with sparse rewards for several simulated tasks,
  outperforming standard reinforcement learning methods and prior methods for hierarchical
  reasoning, model-based planning, and exploration.
layout: inproceedings
id: co-reyes18a
tex_title: 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning
  with Trajectory Embeddings'
firstpage: 1008
lastpage: 1017
page: 1008-1017
order: 1008
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Co-Reyes, John and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin
  and Abbeel, Pieter and Levine, Sergey
author:
- given: John
  family: Co-Reyes
- given: YuXuan
  family: Liu
- given: Abhishek
  family: Gupta
- given: Benjamin
  family: Eysenbach
- given: Pieter
  family: Abbeel
- given: Sergey
  family: Levine
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v80/co-reyes18a/co-reyes18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
