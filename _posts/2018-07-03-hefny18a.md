---
title: Recurrent Predictive State Policy Networks
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/hefny18a/hefny18a.pdf
url: http://proceedings.mlr.press/v80/hefny2018a.html
abstract: We introduce Recurrent Predictive State Policy(RPSP) networks, a recurrent
  architecture that brings insights from predictive state representations to reinforcement
  learning in partially ob-servable environments. Predictive state policy networks
  consist of a recursive filter, which keeps track of a belief about the state of
  the environment, and a reactive policy that directly maps beliefs to actions, to
  maximize the cumulative reward. The recursive filter leverages predictive state
  representations (PSRs) (Rosencrantz & Gordon, 2004; Sun et al., 2016) by modeling
  predictive stateâ€”a prediction of the distribution of future observations conditioned
  on history and future actions.This representation gives rise to a rich class of
  statistically consistent algorithms (Hefny et al.,2017) to initialize the recursive
  filter. Predictive stats serves as an equivalent representation of a belief state.
  Therefore, the policy component of the RPSP-network can be purely reactive, simplifying
  training while still allowing optimal behavior. Moreover, we use the PSR interpretation
  during training as well, by incorporating prediction error in the loss function.
  The entire network (recursive filter and reactive policy) is still differentiable
  and can be trained using gradient-based methods. We optimize our policy using a
  combination of policy gradient based on rewards (Williams, 1992)and gradient descent
  based on prediction error.We show the efficacy of RPSP-networks on a set of robotic
  control tasks from OpenAI Gym. We empirically show that RPSP-networks perform well
  compared with memory-preserving networks such as GRUs, as well as finite memory
  models, being the overall best performing method.
layout: inproceedings
id: hefny18a
tex_title: Recurrent Predictive State Policy Networks
firstpage: 1954
lastpage: 1963
page: 1954-1963
order: 1954
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Hefny, Ahmed and Marinho, Zita and Sun, Wen and Srinivasa, Siddhartha
  and Gordon, Geoffrey
author:
- given: Ahmed
  family: Hefny
- given: Zita
  family: Marinho
- given: Wen
  family: Sun
- given: Siddhartha
  family: Srinivasa
- given: Geoffrey
  family: Gordon
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v80/hefny18a/hefny18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
