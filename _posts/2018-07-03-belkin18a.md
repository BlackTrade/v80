---
title: To Understand Deep Learning We Need to Understand Kernel Learning
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf
url: http://proceedings.mlr.press/v80/belkin2018a.html
abstract: Generalization performance of classifiers in deep learning has recently
  become a subject of intense study. Deep models, which are typically heavily over-parametrized,
  tend to fit the training data exactly. Despite this “overfitting", they perform
  well on test data, a phenomenon not yet fully understood. The first point of our
  paper is that strong performance of overfitted classifiers is not a unique feature
  of deep learning. Using six real-world and two synthetic datasets, we establish
  experimentally that kernel machines trained to have zero classification error or
  near zero regression error (interpolation) perform very well on test data. We proceed
  to give a lower bound on the norm of zero loss solutions for smooth kernels, showing
  that they increase nearly exponentially with data size. None of the existing bounds
  produce non-trivial results for interpolating solutions. We also show experimentally
  that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels
  results recently reported for ReLU neural networks. In contrast, fitting noisy data
  requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted
  Laplacian and Gaussian classifiers on test, suggests that generalization is tied
  to the properties of the kernel function rather than the optimization process. Some
  key phenomena of deep learning are manifested similarly in kernel methods in the
  modern “overfitted" regime. The combination of the experimental and theoretical
  results presented in this paper indicates a need for new theoretical ideas for understanding
  properties of classical kernel methods. We argue that progress on understanding
  deep learning will be difficult until more tractable “shallow” kernel methods are
  better understood.
layout: inproceedings
id: belkin18a
tex_title: To Understand Deep Learning We Need to Understand Kernel Learning
firstpage: 541
lastpage: 549
page: 541-549
order: 541
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik
author:
- given: Mikhail
  family: Belkin
- given: Siyuan
  family: Ma
- given: Soumik
  family: Mandal
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
