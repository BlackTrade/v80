---
title: Learning to Explore via Meta-Policy Gradient
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/xu18d/xu18d.pdf
url: http://proceedings.mlr.press/v80/xu2018d.html
abstract: The performance of off-policy learning, including deep Q-learning and deep
  deterministic policy gradient (DDPG), critically depends on the choice of the exploration
  policy. Existing exploration methods are mostly based on adding noise to the on-going
  actor policy and can only explore <em>local</em> regions close to what the actor
  policy dictates. In this work, we develop a simple meta-policy gradient algorithm
  that allows us to adaptively learn the exploration policy in DDPG. Our algorithm
  allows us to train flexible exploration behaviors that are independent of the actor
  policy, yielding a <em>global exploration</em> that significantly speeds up the
  learning process. With an extensive study, we show that our method significantly
  improves the sample-efficiency of DDPG on a variety of reinforcement learning continuous
  control tasks.
layout: inproceedings
id: xu18d
tex_title: Learning to Explore via Meta-Policy Gradient
firstpage: 5463
lastpage: 5472
page: 5463-5472
order: 5463
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Xu, Tianbing and Liu, Qiang and Zhao, Liang and Peng, Jian
author:
- given: Tianbing
  family: Xu
- given: Qiang
  family: Liu
- given: Liang
  family: Zhao
- given: Jian
  family: Peng
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
