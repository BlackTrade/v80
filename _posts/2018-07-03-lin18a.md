---
title: Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/lin18a/lin18a.pdf
url: http://proceedings.mlr.press/v80/lin2018a.html
abstract: We study generalization properties of distributed algorithms in the setting
  of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We investigate
  distributed stochastic gradient methods (SGM), with mini-batches and multi-passes
  over the data. We show that optimal generalization error bounds can be retained
  for distributed SGM provided that the partition level is not too large. Our results
  are superior to the state-of-the-art theory, covering the cases that the regression
  function may not be in the hypothesis spaces. Particularly, our results show that
  distributed SGM has a smaller theoretical computational complexity, compared with
  distributed kernel ridge regression (KRR) and classic SGM.
layout: inproceedings
id: lin18a
tex_title: Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods
firstpage: 3098
lastpage: 3107
page: 3098-3107
order: 3098
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Lin, Junhong and Cevher, Volkan
author:
- given: Junhong
  family: Lin
- given: Volkan
  family: Cevher
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v80/lin18a/lin18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
