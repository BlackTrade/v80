---
title: Asynchronous Decentralized Parallel Stochastic Gradient Descent
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/lian18a/lian18a.pdf
url: http://proceedings.mlr.press/v80/lian2018a.html
abstract: Most commonly used distributed machine learning systems are either synchronous
  or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly
  in a heterogeneous environment, while asynchronous algorithms using a parameter
  server suffer from 1) communication bottleneck at parameter servers when workers
  are many, and 2) significantly worse convergence when the traffic to parameter server
  is congested. Can we design an algorithm that is robust in a heterogeneous environment,
  while being communication efficient and maintaining the best-possible convergence
  rate? In this paper, we propose an asynchronous decentralized stochastic gradient
  decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis
  shows AD-PSGD converges at the optimal $O(1/\sqrtK)$ rate as SGD and has linear
  speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized
  parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel
  SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment.
  When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t
  epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8x faster
  than its synchronous counterparts in a network-sharing HPC environment. To the best
  of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar
  epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.
layout: inproceedings
id: lian18a
tex_title: Asynchronous Decentralized Parallel Stochastic Gradient Descent
firstpage: 3049
lastpage: 3058
page: 3049-3058
order: 3049
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji
author:
- given: Xiangru
  family: Lian
- given: Wei
  family: Zhang
- given: Ce
  family: Zhang
- given: Ji
  family: Liu
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v80/lian18a/lian18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
