---
title: Understanding Generalization and Optimization Performance of Deep CNNs
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/zhou18a/zhou18a.pdf
url: http://proceedings.mlr.press/v80/zhou2018a.html
abstract: This work aims to provide understandings on the remarkable success of deep
  convolutional neural networks (CNNs) by theoretically analyzing their generalization
  performance and establishing optimization guarantees for gradient descent based
  training algorithms.Specifically, for a CNN model consisting of $l$ convolutional
  layers and one fully connected layer, we prove that its generalization error is
  bounded by $\mathcal{O}(\sqrt{\dt\widetilde{\varrho}/n})$ where $\theta$ denotes
  freedom degree of the network parameters and$\widetilde{\varrho}=\mathcal{O}(\log(\prod_{i=1}^{l}\rwi{i}
  (k_i{i}-s_i{i}+1)/p)+\log(b_{l+1}))$ encapsulates architecture parameters including
  the kernel size $k_i$ stride $s_i$ pooling size $p$ and parameter magnitude $b_i$
  To our best knowledge, this is the first generalization bound that only depends
  on $\mathcal{O}(\log(\prod_{i=1}^{l+1}b_i))$, tighter than existing ones that all
  involve an exponential term like $\mathcal{O}(\prod_{i=1}^{l+1}\rwi{i})$.Besides,
  we prove that for an arbitrary gradient descent algorithm, the computed approximate
  stationary point by minimizing empirical risk is also an approximate stationary
  point to the population risk. This well explains why gradient descent training algorithms
  usually perform sufficiently well in practice. Furthermore, we prove the one-to-one
  correspondence and convergence guarantees for the non-degenerate stationary points
  between the empirical and population risks. It implies that the computed local minimum
  for the empirical risk is also close to a local minimum for the population risk,
  thus ensuring that the optimized CNN model well generalizes to new data.
layout: inproceedings
id: zhou18a
tex_title: Understanding Generalization and Optimization Performance of Deep {CNN}s
firstpage: 5955
lastpage: 5964
page: 5955-5964
order: 5955
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Zhou, Pan and Feng, Jiashi
author:
- given: Pan
  family: Zhou
- given: Jiashi
  family: Feng
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v80/zhou18a/zhou18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
