---
title: Communication-Computation Efficient Gradient Coding
booktitle: Proceedings of the 35th International Conference on Machine Learning
year: '2018'
volume: '80'
series: Proceedings of Machine Learning Research
address: 
month: 0
publisher: PMLR
pdf: http://proceedings.mlr.press/v80/ye18a/ye18a.pdf
url: http://proceedings.mlr.press/v80/ye2018a.html
abstract: 'This paper develops coding techniques to reduce the running time of distributed
  learning tasks. It characterizes the fundamental tradeoff to compute gradients in
  terms of three parameters: computation load, straggler tolerance and communication
  cost. It further gives an explicit coding scheme that achieves the optimal tradeoff
  based on recursive polynomial constructions, coding both across data subsets and
  vector components. As a result, the proposed scheme allows to minimize the running
  time for gradient computations. Implementations are made on Amazon EC2 clusters
  using Python with mpi4py package. Results show that the proposed scheme maintains
  the same generalization error while reducing the running time by $32%$ compared
  to uncoded schemes and $23%$ compared to prior coded schemes focusing only on stragglers
  (Tandon et al., ICML 2017).'
layout: inproceedings
id: ye18a
tex_title: Communication-Computation Efficient Gradient Coding
firstpage: 5610
lastpage: 5619
page: 5610-5619
order: 5610
cycles: false
bibtex_editor: Dy, Jennifer and Krause, Andreas
editor:
- given: Jennifer
  family: Dy
- given: Andreas
  family: Krause
bibtex_author: Ye, Min and Abbe, Emmanuel
author:
- given: Min
  family: Ye
- given: Emmanuel
  family: Abbe
date: 2018-07-03
container-title: Proceedings of the 35th International Conference on Machine Learning
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 7
  - 3
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
